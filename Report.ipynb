{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Nanodegree: Project 3 - Collaboration and Competition - Report\n",
    "\n",
    "\n",
    "### 1. General:\n",
    "\n",
    "The goal of this project was to train two agents to control rackets to bounce a ball over a net. In order to maximize the reward, the agents need to learn how to hit the ball over the net and also how to avoid to let the ball hit the ground or fly out of bounds.\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "<br>\n",
    "Random Agent:\n",
    "\n",
    "[image1]: https://raw.githubusercontent.com/cpow-89/Deep_Reinforcement_Learning_Nanodegree_Project_3_Collaboration_and_Competition/master/images/untrained_agent.gif?token=AmwnwtKri9y4IVzUu-oIN1yMjon4U0fIks5b4fCYwA%3D%3D \"Random Agent\"\n",
    "\n",
    "![Random Agent][image1]\n",
    "\n",
    "### 2. Learning algorithm\n",
    "\n",
    "General Information:\n",
    "\n",
    "The used learning algorithm is a simplified version of the deep genetic algorithm introduced in the paper\n",
    " \"Deep Neuroevolution: Genetic Algorithms are a Competitive Alternative forTraining Deep Neural Networks for Reinforcement Learning\" by Uber AI Labs. My version does not implement the multiprocessing component of this paper. I also did not implement Novelty Search cause the basic algorithm was already able to solve the task.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. Initialization:\n",
    "- create a initial population $P$ of $N$ individuals\n",
    "    - in our case, an individual is defined as the weights $\\theta$ of a neural network \n",
    "    - every individual is evaluated based on a fitness function \n",
    "    - we save one individual as a tuple in the form of ($\\theta$, fitness_score)\n",
    "    \n",
    "2. Evolution:\n",
    "- sort current population by fitness\n",
    "- evolves a population $P$ of $N$ individuals \n",
    "- generate the next generation\n",
    "    - the best individual from the current generation is copied unchanged to the next generation\n",
    "        - this technique is called elitism\n",
    "        - to more reliably try to select the true elite, we evaluate each of the top n individuals per generation on x additional episodes and calculate the mean reward\n",
    "        - the best individual is then selected as elite\n",
    "        - n is defined in config as \"parent_count\"\n",
    "        - x is defined in config as \"elite_evaluation_count\"\n",
    "    - evaluate the parents for the next generation via truncation selection \n",
    "        - the top $T$ individuals become the parents of the next generation\n",
    "        - we select uniformly at random a parameter $\\theta_i$ from the selected parents\n",
    "        - we mutated the selected parent by applying additive Gaussian noise $\\delta$ to the parameter $\\theta_i$ \n",
    "            - $\\theta' = \\theta + \\delta$ \n",
    "        - repeated $N - 1$ times\n",
    "- repeat the process until task is solved\n",
    "\n",
    "\n",
    "### 3. Hyperparameters\n",
    "- hyperparameters can be found in the config file\n",
    "\n",
    "    \n",
    "observation_size: 48\n",
    "- agent observation size is 48 cause the policy network gets the input of 2 agents with respectively 24 input signals\n",
    "\n",
    "action_size: 4\n",
    "- agent action size is 4 cause the policy network controls two agents with respectively 2 action signals\n",
    "\n",
    "fc_units: [ 64, 64, 64 ]\n",
    "- the policy network is build up of 3 feed-forward layers\n",
    "- the list determines the number of hidden nodes for each layer\n",
    "- the number of nodes was chosen experimentally\n",
    "\n",
    "activation_funcs: [ \"Tanh\", \"Tanh\", \"Tanh\" ]\n",
    "- the policy network is build up of 3 feed-forward layers\n",
    "- the list determines the type of activation function to be used for the output of any layer\n",
    "- Tanh was used cause the agents need to make continuous actions in a range of -1.0 to 1.0\n",
    "\n",
    "noise_std: 0.01\n",
    "- the standard deviation for the Gaussian noised used to mutated the population policy network parameters\n",
    "\n",
    "population_size: 50\n",
    "- number $n$ of policy networks to create to build a population $P$ for the deep genetic algorithm to operate on\n",
    "- the value was chosen to be large enough to get a reasonably great variance in the population but also be not to heavy on the computational side\n",
    "\n",
    "parent_count: 10\n",
    "- number $T$ of top performing individuals that become the parents of the next generation\n",
    "\n",
    "elite_evaluation_count: 5\n",
    "- number of evaluations made for every top $T$ individual during elitism\n",
    "- in the original paper, this number is set to 30, but this was way too time-consuming for this a non-multiprocessing version\n",
    "\n",
    "### 4. Network architectures\n",
    "\n",
    "ContinuousPolicyNetwork(<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(network): Sequential(<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0): Linear(in_features=48, out_features=64, bias=True)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1): Tanh()<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2): Linear(in_features=64, out_features=64, bias=True)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3): Tanh()<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4): Linear(in_features=64, out_features=4, bias=True)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(5): Tanh()<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;)<br>\n",
    ")<br>\n",
    "\n",
    "### 5. Results\n",
    "\n",
    "Environment solved in 106 generations.    Average reward (over 100 episodes): 0.7\n",
    "\n",
    "To get a better view of the training session statistic open the tensorboard log with the following console command:\n",
    "tensorboard --logdir=monitor/Tennis_Linux/2018_10_29__21_37_16\n",
    "\n",
    "[image2]: https://raw.githubusercontent.com/cpow-89/Deep_Reinforcement_Learning_Nanodegree_Project_3_Collaboration_and_Competition/master/images/reward%20plot.png?token=AmwnwqPxOkm7MIWmWfE2gtiHLjrcoh1eks5b4edqwA%3D%3D \"Reward Plot\"\n",
    "![Trained Agent][image2]\n",
    "\n",
    "- i stopped training before the reward function reached the maximum cause of long computational time\n",
    "\n",
    "\n",
    "\n",
    "Trained Agent:\n",
    "\n",
    "[image3]: https://raw.githubusercontent.com/cpow-89/Deep_Reinforcement_Learning_Nanodegree_Project_3_Collaboration_and_Competition/master/images/trained_agent.gif?token=Amwnwiow3R4NuNeRDGtNGVjxiVDyXnWEks5b4e96wA%3D%3D \"Trained Agent\"\n",
    "![Trained Agent][image3]\n",
    "\n",
    "\n",
    "\n",
    "### 6. Ideas for Future Work\n",
    "- add Novelty Search to get more variance into the population to avoid local minima\n",
    "- add multiprocessing -> currently not possible for the given unity environment provided by udacity\n",
    "    - at least the linux version of the environment was not multiprocessing ready\n",
    "    - this seems to be a known issue: https://github.com/Unity-Technologies/ml-agents/issues/956\n",
    "- try different selection functions like \"tournament\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
